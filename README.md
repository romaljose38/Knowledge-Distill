# Knowledge-Distill
Implementation of Knowledge Distillation using Tensorflow

# Knowledge Distillation with TensorFlow

## Overview
This repository showcases the implementation of knowledge distillation using TensorFlow. The parent model is based on VGG16, serving as the teacher, while a custom student model is created and trained to distill knowledge from the teacher.

## Key Features
- Knowledge distillation from VGG16 to a custom student model
- TensorFlow implementation for easy usage and understanding
- Evaluation of distilled model performance compared to the teacher

## License
This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.
